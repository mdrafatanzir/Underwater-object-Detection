{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32404,"status":"ok","timestamp":1768947001600,"user":{"displayName":"rafat anzir","userId":"17104502223710715055"},"user_tz":-120},"id":"W8vGVF0eb7aS","outputId":"8ff61e51-3c8a-4726-d517-5f60af1282a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (2.0.11)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n","Requirement already satisfied: typing-extensions\u003e=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx\u003e=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec\u003e=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib) (1.17.0)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch) (3.0.3)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install torch torchvision opencv-python matplotlib numpy tqdm pycocotools\n","\n","import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.transforms import functional as F\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCISDk52b_-5"},"outputs":[],"source":["\n","BASE_PATH = \"/content/drive/My Drive/Colab Notebooks/Thesis_Dataset/Dataset1\"\n","TRAIN_IMG = f\"{BASE_PATH}/train/images\"\n","TRAIN_LBL = f\"{BASE_PATH}/train/labels\"\n","TEST_IMG  = f\"{BASE_PATH}/test/images\"\n","TEST_LBL  = f\"{BASE_PATH}/test/labels\"\n","\n","CLASSES = [\"propeller\",\"pipe_type2\",\"red_fin\",\"net\",\"qr_codes\",\"pipe\"]\n","NUM_CLASSES = len(CLASSES) + 1  # + background\n","\n","def read_yolo_labels(txt_file, W, H):\n","    boxes, labels = [], []\n","    if not os.path.exists(txt_file):\n","        return torch.zeros((0,4)), torch.zeros((0,), dtype=torch.int64)\n","\n","    with open(txt_file) as f:\n","        for line in f:\n","            c,x,y,w,h = map(float, line.split())\n","            x1 = (x - w/2) * W\n","            y1 = (y - h/2) * H\n","            x2 = (x + w/2) * W\n","            y2 = (y + h/2) * H\n","            boxes.append([x1,y1,x2,y2])\n","            labels.append(int(c)+1)\n","\n","    return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n","\n","class YoloDataset(Dataset):\n","    def __init__(self, img_dir, lbl_dir):\n","        self.img_dir = img_dir\n","        self.lbl_dir = lbl_dir\n","        self.images = [x for x in os.listdir(img_dir) if x.endswith(\".jpg\")]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.images[idx]\n","        img_path = os.path.join(self.img_dir, img_name)\n","        lbl_path = os.path.join(self.lbl_dir, img_name.replace(\".jpg\",\".txt\"))\n","\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        H,W,_ = img.shape\n","\n","        boxes, labels = read_yolo_labels(lbl_path, W, H)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","\n","        img = F.to_tensor(img)\n","        return img, target\n","\n","train_ds = YoloDataset(TRAIN_IMG, TRAIN_LBL)\n","test_ds  = YoloDataset(TEST_IMG, TEST_LBL)\n","\n","train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,\n","                          collate_fn=lambda x: tuple(zip(*x)))\n","test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False,\n","                          collate_fn=lambda x: tuple(zip(*x)))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDQjg2W8cQmX","outputId":"09bcafb7-5fcf-4c4a-bcb7-efde137cf41d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 160M/160M [00:00\u003c00:00, 200MB/s]\n","100%|██████████| 1267/1267 [27:36\u003c00:00,  1.31s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5 Loss: 0.20476899881448066\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 200/1267 [02:10\u003c11:39,  1.52it/s]"]}],"source":["# -------- MODEL --------\n","model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# -------- TRAIN --------\n","EPOCHS = 5\n","for epoch in range(EPOCHS):\n","    model.train()\n","    total_loss = 0\n","\n","    for imgs, targets in tqdm(train_loader):\n","        imgs = [img.to(device) for img in imgs]\n","        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n","\n","        loss_dict = model(imgs, targets)\n","        loss = sum(v for v in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{EPOCHS} Loss:\", total_loss/len(train_loader))\n","\n","torch.save(model.state_dict(), \"/content/faster_rcnn_fixed.pth\")\n","print(\"✔ Faster R-CNN trained\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtNlUtUlcTeu"},"outputs":[],"source":["\n","# -------- IoU --------\n","def IoU(a,b):\n","    xA=max(a[0],b[0]); yA=max(a[1],b[1])\n","    xB=min(a[2],b[2]); yB=min(a[3],b[3])\n","    inter=max(0,xB-xA)*max(0,yB-yA)\n","    areaA=(a[2]-a[0])*(a[3]-a[1])\n","    areaB=(b[2]-b[0])*(b[3]-b[1])\n","    return inter/(areaA+areaB-inter) if areaA+areaB-inter else 0\n","\n","# -------- TEST --------\n","model.eval()\n","correct = 0\n","total = 0\n","\n","for imgs, targets in tqdm(test_loader):\n","    img = imgs[0].to(device)\n","    gt_boxes = targets[0][\"boxes\"].cpu().numpy()\n","    gt_labels = targets[0][\"labels\"].cpu().numpy()\n","\n","    preds = model([img])[0]\n","    boxes = preds[\"boxes\"].cpu().numpy()\n","    labels = preds[\"labels\"].cpu().numpy()\n","\n","    for b,l in zip(boxes,labels):\n","        total += 1\n","        for gb,gl in zip(gt_boxes,gt_labels):\n","            if l==gl and IoU(b,gb)\u003e=0.5:\n","                correct += 1\n","                break\n","\n","print(\"Faster R-CNN Detection Accuracy (%):\", correct/total*100)\n","\n","# -------- VISUAL --------\n","img_name = os.listdir(TEST_IMG)[0]\n","img = cv2.imread(os.path.join(TEST_IMG, img_name))\n","H,W,_ = img.shape\n","img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","lbl_path = os.path.join(TEST_LBL, img_name.replace(\".jpg\",\".txt\"))\n","gt_boxes,_ = read_yolo_labels(lbl_path, W, H)\n","\n","for b in gt_boxes:\n","    x1,y1,x2,y2 = map(int,b)\n","    cv2.rectangle(img_rgb,(x1,y1),(x2,y2),(0,0,255),2)\n","\n","pred = model([F.to_tensor(img_rgb).to(device)])[0]\n","for b in pred[\"boxes\"].cpu().numpy().astype(int):\n","    x1,y1,x2,y2 = b\n","    cv2.rectangle(img_rgb,(x1,y1),(x2,y2),(0,255,0),2)\n","\n","plt.imshow(img_rgb)\n","plt.title(\"Faster R-CNN: GT (Red) vs Pred (Green)\")\n","plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35560,"status":"ok","timestamp":1768984463039,"user":{"displayName":"rafat anzir","userId":"17104502223710715055"},"user_tz":-120},"id":"leu7zOa0r26_","outputId":"2b689e9b-f621-430f-a5dc-c42deb40ee8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Setup\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q torch torchvision opencv-python matplotlib numpy tqdm pycocotools\n","\n","import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.transforms import functional as F\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1768984463071,"user":{"displayName":"rafat anzir","userId":"17104502223710715055"},"user_tz":-120},"id":"crclCdFIsDrK"},"outputs":[],"source":["\n","# Paths \u0026 Classes\n","BASE_PATH = \"/content/drive/My Drive/Colab Notebooks/Thesis_Dataset/Dataset1\"\n","TRAIN_IMG = f\"{BASE_PATH}/train/images\"\n","TRAIN_LBL = f\"{BASE_PATH}/train/labels\"\n","TEST_IMG  = f\"{BASE_PATH}/test/images\"\n","TEST_LBL  = f\"{BASE_PATH}/test/labels\"\n","\n","CLASSES = [\"propeller\",\"pipe_type2\",\"red_fin\",\"net\",\"qr_codes\",\"pipe\"]\n","NUM_CLASSES = len(CLASSES) + 1  # + background\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1768984503744,"user":{"displayName":"rafat anzir","userId":"17104502223710715055"},"user_tz":-120},"id":"vJeudgWasKCo"},"outputs":[],"source":["# Helpers\n","def read_yolo_labels(txt_file, W, H):\n","    boxes, labels = [], []\n","    if not os.path.exists(txt_file):\n","        return torch.zeros((0,4)), torch.zeros((0,), dtype=torch.int64)\n","\n","    with open(txt_file) as f:\n","        for line in f:\n","            c,x,y,w,h = map(float, line.split())\n","            x1 = (x - w/2) * W\n","            y1 = (y - h/2) * H\n","            x2 = (x + w/2) * W\n","            y2 = (y + h/2) * H\n","            boxes.append([x1,y1,x2,y2])\n","            labels.append(int(c)+1)\n","\n","    return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n","\n","def IoU(a,b):\n","    xA=max(a[0],b[0]); yA=max(a[1],b[1])\n","    xB=min(a[2],b[2]); yB=min(a[3],b[3])\n","    inter=max(0,xB-xA)*max(0,yB-yA)\n","    areaA=(a[2]-a[0])*(a[3]-a[1])\n","    areaB=(b[2]-b[0])*(b[3]-b[1])\n","    return inter/(areaA+areaB-inter) if areaA+areaB-inter else 0\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8154,"status":"ok","timestamp":1768984534208,"user":{"displayName":"rafat anzir","userId":"17104502223710715055"},"user_tz":-120},"id":"dae4BNPUsW3h"},"outputs":[],"source":["# Dataset\n","class YoloDataset(Dataset):\n","    def __init__(self, img_dir, lbl_dir):\n","        self.img_dir = img_dir\n","        self.lbl_dir = lbl_dir\n","        self.images = sorted([x for x in os.listdir(img_dir) if x.endswith(\".jpg\")])\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.images[idx]\n","        img_path = os.path.join(self.img_dir, img_name)\n","        lbl_path = os.path.join(self.lbl_dir, img_name.replace(\".jpg\",\".txt\"))\n","\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        H,W,_ = img.shape\n","\n","        boxes, labels = read_yolo_labels(lbl_path, W, H)\n","\n","        target = {\"boxes\": boxes, \"labels\": labels}\n","        img = F.to_tensor(img)\n","        return img, target\n","\n","train_ds = YoloDataset(TRAIN_IMG, TRAIN_LBL)\n","test_ds  = YoloDataset(TEST_IMG, TEST_LBL)\n","\n","train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,\n","                          collate_fn=lambda x: tuple(zip(*x)))\n","test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False,\n","                          collate_fn=lambda x: tuple(zip(*x)))\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2014,"status":"ok","timestamp":1768984548854,"user":{"displayName":"rafat anzir","userId":"17104502223710715055"},"user_tz":-120},"id":"H5u3N5WCschl","outputId":"f1c44ff8-11fa-4cfa-c5eb-48261c92e661"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 160M/160M [00:00\u003c00:00, 195MB/s]\n","/tmp/ipython-input-3720854460.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]}],"source":["#Model\n","model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n","\n","# Speed optimization\n","model.rpn.pre_nms_top_n_train = 1000\n","model.rpn.post_nms_top_n_train = 500\n","model.rpn.pre_nms_top_n_test = 1000\n","model.rpn.post_nms_top_n_test = 500\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","scaler = torch.cuda.amp.GradScaler()\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2952368,"status":"ok","timestamp":1768988032324,"user":{"displayName":"rafat anzir","userId":"17104502223710715055"},"user_tz":-120},"id":"t4UoBugBsgfg"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1267 [00:00\u003c?, ?it/s]/tmp/ipython-input-3486175642.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","100%|██████████| 1267/1267 [23:44\u003c00:00,  1.12s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5 Loss: 0.19955802481289256\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1267/1267 [08:23\u003c00:00,  2.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/5 Loss: 0.12588097210370752\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1267/1267 [08:24\u003c00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/5 Loss: 0.10765503381908928\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1267/1267 [08:24\u003c00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/5 Loss: 0.10177261923934497\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1267/1267 [08:25\u003c00:00,  2.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/5 Loss: 0.090919384919548\n","✔ Faster R-CNN trained\n"]}],"source":["#  Training\n","EPOCHS = 5\n","for epoch in range(EPOCHS):\n","    model.train()\n","    total_loss = 0\n","\n","    for imgs, targets in tqdm(train_loader):\n","        imgs = [img.to(device) for img in imgs]\n","        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n","\n","        with torch.cuda.amp.autocast():\n","            loss_dict = model(imgs, targets)\n","            loss = sum(v for v in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{EPOCHS} Loss:\", total_loss/len(train_loader))\n","\n","torch.save(model.state_dict(), \"/content/faster_rcnn_final.pth\")\n","print(\"✔ Faster R-CNN trained\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rpRpLLJsu-S","outputId":"b53ab085-d356-4c44-8ca6-5856c7f21784"},"outputs":[{"name":"stderr","output_type":"stream","text":["  2%|▏         | 7/318 [00:05\u003c03:56,  1.31it/s]"]}],"source":["\n","# Evaluation Metrics\n","\n","model.eval()\n","\n","TP, FP, FN = 0, 0, 0\n","correct, total = 0, 0\n","\n","for imgs, targets in tqdm(test_loader):\n","    img = imgs[0].to(device)\n","    gt_boxes = targets[0][\"boxes\"].cpu().numpy()\n","    gt_labels = targets[0][\"labels\"].cpu().numpy()\n","\n","    with torch.no_grad():\n","        preds = model([img])[0]\n","\n","    boxes = preds[\"boxes\"].cpu().numpy()\n","    labels = preds[\"labels\"].cpu().numpy()\n","    scores = preds[\"scores\"].cpu().numpy()\n","\n","    matched_gt = set()\n","\n","    for b,l,s in zip(boxes,labels,scores):\n","        if s \u003c 0.5:\n","            continue\n","\n","        total += 1\n","        matched = False\n","        for i,(gb,gl) in enumerate(zip(gt_boxes,gt_labels)):\n","            if i in matched_gt:\n","                continue\n","            if l==gl and IoU(b,gb)\u003e=0.5:\n","                TP += 1\n","                correct += 1\n","                matched_gt.add(i)\n","                matched = True\n","                break\n","        if not matched:\n","            FP += 1\n","\n","    FN += len(gt_boxes) - len(matched_gt)\n","\n","accuracy = correct / total * 100 if total else 0\n","precision = TP / (TP + FP) * 100 if TP+FP else 0\n","recall = TP / (TP + FN) * 100 if TP+FN else 0\n","map50 = precision  # IoU≥0.5 approximation\n","\n","print(\"\\n Faster R-CNN Metrics\")\n","print(\"Accuracy (%):\", round(accuracy,2))\n","print(\"Precision (%):\", round(precision,2))\n","print(\"Recall (%):\", round(recall,2))\n","print(\"mAP@0.5 (%):\", round(map50,2))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cB_OLkXtsxze"},"outputs":[],"source":["# Visualization\n","sample_imgs = os.listdir(TEST_IMG)[:5]\n","\n","for img_name in sample_imgs:\n","    img = cv2.imread(os.path.join(TEST_IMG, img_name))\n","    H,W,_ = img.shape\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    # GT (BLUE)\n","    lbl_path = os.path.join(TEST_LBL, img_name.replace(\".jpg\",\".txt\"))\n","    gt_boxes,_ = read_yolo_labels(lbl_path, W, H)\n","    for b in gt_boxes:\n","        x1,y1,x2,y2 = map(int,b)\n","        cv2.rectangle(img_rgb,(x1,y1),(x2,y2),(0,0,255),2)\n","\n","    # Predictions (GREEN)\n","    with torch.no_grad():\n","        pred = model([F.to_tensor(img_rgb).to(device)])[0]\n","\n","    for b,s in zip(pred[\"boxes\"].cpu().numpy(), pred[\"scores\"].cpu().numpy()):\n","        if s \u003c 0.5:\n","            continue\n","        x1,y1,x2,y2 = map(int,b)\n","        cv2.rectangle(img_rgb,(x1,y1),(x2,y2),(0,255,0),2)\n","\n","    plt.figure(figsize=(8,8))\n","    plt.imshow(img_rgb)\n","    plt.title(\"Faster R-CNN: GT (Blue) vs Pred (Green)\")\n","    plt.axis(\"off\")\n","    plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP4zHB5PiKsvYLPI3wr0Aii","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}